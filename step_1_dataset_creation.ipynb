{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "\n",
    "HOST = 'https://bookmix.ru'\n",
    "URL = 'https://bookmix.ru/comments/'\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Here was my user-agent address'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a func that gets url with user-agent and 0 parameters\n",
    "def get_html(url, params=''):\n",
    "  response = requests.get(url, headers=HEADERS, params=params)\n",
    "  return response\n",
    "\n",
    "# a func that acquires full version of reviews via \"read more\" links\n",
    "def get_full_reviews(soup):\n",
    "  all_comments_ids = []\n",
    "  ids_only = []\n",
    "  full_reviews = []\n",
    "\n",
    "  # getting links to comments and removing the same ones with an ordered dict\n",
    "  links = (link.get('href') for link in soup.findAll('a') if '/discussion.phtml?id=' in str(link.get('href')))\n",
    "  links_unique = list(OrderedDict.fromkeys(links))\n",
    "  links_unique = [i for i in links_unique if 'comment' in i]\n",
    "\n",
    "  # getting all the #comment_ids\n",
    "  for url in links_unique:\n",
    "    all_comments_ids.append(re.search('#comment\\d+', url)[0])\n",
    "\n",
    "  # getting all the ids themselves\n",
    "  for comment_id in all_comments_ids:\n",
    "    ids_only.append(re.search('\\d+', comment_id)[0])\n",
    "\n",
    "  # extracting full-length book reviews based on their ids\n",
    "  for link, comment_id in zip(links_unique, ids_only):\n",
    "    response = get_html(HOST + link)\n",
    "    soup = bs(response.content, 'lxml', from_encoding='utf-8')\n",
    "    try:\n",
    "        item = soup.find('div', class_=\"item-comment level1\", id='comment'+comment_id).find(class_=\n",
    "                                                 'comment-content').get_text(strip=True)\n",
    "        full_reviews.append(item)\n",
    "    except AttributeError or ChunkedEncodingError or ProtocolError or ValueError or IncompleteRead:\n",
    "        pass\n",
    "  return full_reviews\n",
    "\n",
    "'''\n",
    "a func that converts \"rating stars\" to labels (negative, neutral, positive) for negative (2 stars or lower),\n",
    "neutral (3 stars), and positive (4 stars or higher) reviews respectively\n",
    "'''\n",
    "def get_sentiments(items):\n",
    "  raw_data = []\n",
    "  ratings = []\n",
    "  sentiments = []\n",
    "  for item in items:\n",
    "    raw_data.append({\n",
    "      'rating': item.find('div', {'class': ['rating', 'disabled', 'star[0-5]{1}']})\n",
    "      })\n",
    "  for i in raw_data:\n",
    "    if (re.findall('[0-5]', str(i))) == []:\n",
    "      ratings.append('0')\n",
    "    else:\n",
    "      ratings.append(re.findall('[0-5]', str(i))[0])\n",
    "\n",
    "  for i in ratings:\n",
    "    if int(i) > 3:\n",
    "      sentiments.append('positive')\n",
    "    elif int(i) == 3:\n",
    "      sentiments.append('neutral')\n",
    "    elif int(i) < 3:\n",
    "      sentiments.append('negative')\n",
    "  return sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_reviews = []\n",
    "total_sentiments = []\n",
    "'''\n",
    "a cycle for iterating over the first 10 pages from \"https://bookmix.ru/comments/\" \n",
    "and getting both full reviews and sentiment labels \n",
    "\\\n",
    "since the website was crashing after approximately 500 pages (20 reviews each),\n",
    "we had to parse all the 80 000 reviews in small batches, and put them together in a complete dataset afterwards\n",
    "'''\n",
    "for page in range(1, 501):\n",
    "  print(f\"Parsing page number {page}\")\n",
    "  URL = \"https://bookmix.ru/comments/index.phtml?begin={}&num_point=20&num_points=20\".format(page*20)\n",
    "  response = get_html(URL)\n",
    "  soup = bs(response.content, 'lxml', from_encoding='utf-8')\n",
    "  items = soup.findAll('div', class_=\"universal-blocks\")\n",
    "\n",
    "  page_reviews = get_full_reviews(soup)\n",
    "  page_sentiments = get_sentiments(items)\n",
    "\n",
    "  total_reviews.extend(page_reviews)\n",
    "  total_sentiments.extend(page_sentiments)\n",
    "print(\"Parsing is finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "writing the first batch of 500 pages of book reviews into a file with the respective name\n",
    "'''\n",
    "df = pd.DataFrame(list(zip(total_reviews, total_sentiments)), columns = ['Review', 'Sentiment'])\n",
    "df.to_csv(r'Reviews1-500.csv', sep=',', encoding='utf-8', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "#set working directory\n",
    "os.chdir(\"/Users/urijzuzaev/Desktop/reviews_dataset\")\n",
    "\n",
    "'''\n",
    "finding all the batch names and writing them to a list\n",
    "'''\n",
    "extension = 'csv'\n",
    "all_filenames = [i for i in glob.glob('*.{}'.format(extension))]\n",
    "print(all_filenames)\n",
    "\n",
    "'''\n",
    "finding all the batches parsed during the previous stage and combining them together into one dataset\n",
    "'''\n",
    "combined_csv = pd.concat([pd.read_csv(f) for f in all_filenames ])\n",
    "combined_csv.to_csv(\"combined_dataset.csv\", index=False, encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
